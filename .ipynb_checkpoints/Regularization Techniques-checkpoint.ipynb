{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ae1de2b-d75a-46c2-8037-6c235fdb43aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 6.7069, Val Loss: 5.0102\n",
      "Epoch 1, Train Loss: 6.5053, Val Loss: 4.7570\n",
      "Epoch 2, Train Loss: 6.1274, Val Loss: 4.5262\n",
      "Epoch 3, Train Loss: 5.8693, Val Loss: 4.3067\n",
      "Epoch 4, Train Loss: 5.5021, Val Loss: 4.0971\n",
      "Epoch 5, Train Loss: 5.3790, Val Loss: 3.9013\n",
      "Epoch 6, Train Loss: 4.9178, Val Loss: 3.7222\n",
      "Epoch 7, Train Loss: 4.8988, Val Loss: 3.5420\n",
      "Epoch 8, Train Loss: 4.4178, Val Loss: 3.3831\n",
      "Epoch 9, Train Loss: 4.2595, Val Loss: 3.2180\n",
      "Epoch 10, Train Loss: 4.1767, Val Loss: 3.0583\n",
      "Epoch 11, Train Loss: 4.0214, Val Loss: 2.9098\n",
      "Epoch 12, Train Loss: 3.7574, Val Loss: 2.7732\n",
      "Epoch 13, Train Loss: 3.6414, Val Loss: 2.6385\n",
      "Epoch 14, Train Loss: 3.4068, Val Loss: 2.5080\n",
      "Epoch 15, Train Loss: 3.1310, Val Loss: 2.3756\n",
      "Epoch 16, Train Loss: 3.1123, Val Loss: 2.2555\n",
      "Epoch 17, Train Loss: 3.0098, Val Loss: 2.1400\n",
      "Epoch 18, Train Loss: 2.8519, Val Loss: 2.0299\n",
      "Epoch 19, Train Loss: 2.6951, Val Loss: 1.9272\n",
      "Epoch 20, Train Loss: 2.4628, Val Loss: 1.8239\n",
      "Epoch 21, Train Loss: 2.2935, Val Loss: 1.7306\n",
      "Epoch 22, Train Loss: 2.1780, Val Loss: 1.6410\n",
      "Epoch 23, Train Loss: 2.3398, Val Loss: 1.5601\n",
      "Epoch 24, Train Loss: 2.1209, Val Loss: 1.4834\n",
      "Epoch 25, Train Loss: 1.8941, Val Loss: 1.4114\n",
      "Epoch 26, Train Loss: 1.8842, Val Loss: 1.3443\n",
      "Epoch 27, Train Loss: 1.6224, Val Loss: 1.2795\n",
      "Epoch 28, Train Loss: 1.7251, Val Loss: 1.2170\n",
      "Epoch 29, Train Loss: 1.5714, Val Loss: 1.1678\n",
      "Epoch 30, Train Loss: 1.3503, Val Loss: 1.1137\n",
      "Epoch 31, Train Loss: 1.4100, Val Loss: 1.0572\n",
      "Epoch 32, Train Loss: 1.2475, Val Loss: 1.0084\n",
      "Epoch 33, Train Loss: 1.0145, Val Loss: 0.9591\n",
      "Epoch 34, Train Loss: 1.3932, Val Loss: 0.9166\n",
      "Epoch 35, Train Loss: 1.2750, Val Loss: 0.8735\n",
      "Epoch 36, Train Loss: 1.0104, Val Loss: 0.8369\n",
      "Epoch 37, Train Loss: 1.0330, Val Loss: 0.8051\n",
      "Epoch 38, Train Loss: 1.0721, Val Loss: 0.7732\n",
      "Epoch 39, Train Loss: 1.0264, Val Loss: 0.7445\n",
      "Epoch 40, Train Loss: 0.9466, Val Loss: 0.7189\n",
      "Epoch 41, Train Loss: 0.8816, Val Loss: 0.6965\n",
      "Epoch 42, Train Loss: 1.0386, Val Loss: 0.6707\n",
      "Epoch 43, Train Loss: 0.7262, Val Loss: 0.6526\n",
      "Epoch 44, Train Loss: 0.8243, Val Loss: 0.6269\n",
      "Epoch 45, Train Loss: 0.8736, Val Loss: 0.6080\n",
      "Epoch 46, Train Loss: 0.7970, Val Loss: 0.5889\n",
      "Epoch 47, Train Loss: 0.7671, Val Loss: 0.5695\n",
      "Epoch 48, Train Loss: 0.7231, Val Loss: 0.5489\n",
      "Epoch 49, Train Loss: 0.7284, Val Loss: 0.5316\n",
      "Epoch 50, Train Loss: 0.6319, Val Loss: 0.5139\n",
      "Epoch 51, Train Loss: 0.5883, Val Loss: 0.4959\n",
      "Epoch 52, Train Loss: 0.7270, Val Loss: 0.4862\n",
      "Epoch 53, Train Loss: 0.6314, Val Loss: 0.4703\n",
      "Epoch 54, Train Loss: 0.6217, Val Loss: 0.4543\n",
      "Epoch 55, Train Loss: 0.4858, Val Loss: 0.4460\n",
      "Epoch 56, Train Loss: 0.5592, Val Loss: 0.4306\n",
      "Epoch 57, Train Loss: 0.5446, Val Loss: 0.4176\n",
      "Epoch 58, Train Loss: 0.6984, Val Loss: 0.4076\n",
      "Epoch 59, Train Loss: 0.5683, Val Loss: 0.3978\n",
      "Epoch 60, Train Loss: 0.4250, Val Loss: 0.3921\n",
      "Epoch 61, Train Loss: 0.5894, Val Loss: 0.3791\n",
      "Epoch 62, Train Loss: 0.5574, Val Loss: 0.3718\n",
      "Epoch 63, Train Loss: 0.5292, Val Loss: 0.3619\n",
      "Epoch 64, Train Loss: 0.6254, Val Loss: 0.3568\n",
      "Epoch 65, Train Loss: 0.5040, Val Loss: 0.3499\n",
      "Epoch 66, Train Loss: 0.5676, Val Loss: 0.3433\n",
      "Epoch 67, Train Loss: 0.6092, Val Loss: 0.3344\n",
      "Epoch 68, Train Loss: 0.5577, Val Loss: 0.3280\n",
      "Epoch 69, Train Loss: 0.5857, Val Loss: 0.3235\n",
      "Epoch 70, Train Loss: 0.4111, Val Loss: 0.3165\n",
      "Epoch 71, Train Loss: 0.5135, Val Loss: 0.3070\n",
      "Epoch 72, Train Loss: 0.5718, Val Loss: 0.3042\n",
      "Epoch 73, Train Loss: 0.5624, Val Loss: 0.2966\n",
      "Epoch 74, Train Loss: 0.4687, Val Loss: 0.2943\n",
      "Epoch 75, Train Loss: 0.4504, Val Loss: 0.2875\n",
      "Epoch 76, Train Loss: 0.4622, Val Loss: 0.2769\n",
      "Epoch 77, Train Loss: 0.4848, Val Loss: 0.2731\n",
      "Epoch 78, Train Loss: 0.5809, Val Loss: 0.2704\n",
      "Epoch 79, Train Loss: 0.3723, Val Loss: 0.2682\n",
      "Epoch 80, Train Loss: 0.4557, Val Loss: 0.2673\n",
      "Epoch 81, Train Loss: 0.5290, Val Loss: 0.2723\n",
      "Epoch 82, Train Loss: 0.3750, Val Loss: 0.2687\n",
      "Epoch 83, Train Loss: 0.5064, Val Loss: 0.2633\n",
      "Epoch 84, Train Loss: 0.4109, Val Loss: 0.2605\n",
      "Epoch 85, Train Loss: 0.4291, Val Loss: 0.2577\n",
      "Epoch 86, Train Loss: 0.5114, Val Loss: 0.2587\n",
      "Epoch 87, Train Loss: 0.3977, Val Loss: 0.2527\n",
      "Epoch 88, Train Loss: 0.4171, Val Loss: 0.2517\n",
      "Epoch 89, Train Loss: 0.4461, Val Loss: 0.2475\n",
      "Epoch 90, Train Loss: 0.4783, Val Loss: 0.2432\n",
      "Epoch 91, Train Loss: 0.4752, Val Loss: 0.2352\n",
      "Epoch 92, Train Loss: 0.4745, Val Loss: 0.2321\n",
      "Epoch 93, Train Loss: 0.3313, Val Loss: 0.2297\n",
      "Epoch 94, Train Loss: 0.4269, Val Loss: 0.2293\n",
      "Epoch 95, Train Loss: 0.4412, Val Loss: 0.2363\n",
      "Epoch 96, Train Loss: 0.4123, Val Loss: 0.2303\n",
      "Epoch 97, Train Loss: 0.4592, Val Loss: 0.2295\n",
      "Epoch 98, Train Loss: 0.4550, Val Loss: 0.2254\n",
      "Epoch 99, Train Loss: 0.4310, Val Loss: 0.2241\n",
      "Epoch 100, Train Loss: 0.4852, Val Loss: 0.2238\n",
      "Epoch 101, Train Loss: 0.4692, Val Loss: 0.2194\n",
      "Epoch 102, Train Loss: 0.3706, Val Loss: 0.2190\n",
      "Epoch 103, Train Loss: 0.4253, Val Loss: 0.2171\n",
      "Epoch 104, Train Loss: 0.4443, Val Loss: 0.2213\n",
      "Epoch 105, Train Loss: 0.4310, Val Loss: 0.2213\n",
      "Epoch 106, Train Loss: 0.4015, Val Loss: 0.2198\n",
      "Epoch 107, Train Loss: 0.4169, Val Loss: 0.2144\n",
      "Epoch 108, Train Loss: 0.4800, Val Loss: 0.2182\n",
      "Epoch 109, Train Loss: 0.4862, Val Loss: 0.2132\n",
      "Epoch 110, Train Loss: 0.4146, Val Loss: 0.2101\n",
      "Epoch 111, Train Loss: 0.3604, Val Loss: 0.2046\n",
      "Epoch 112, Train Loss: 0.4524, Val Loss: 0.2052\n",
      "Epoch 113, Train Loss: 0.3848, Val Loss: 0.2047\n",
      "Epoch 114, Train Loss: 0.4064, Val Loss: 0.2051\n",
      "Epoch 115, Train Loss: 0.3957, Val Loss: 0.2023\n",
      "Epoch 116, Train Loss: 0.3785, Val Loss: 0.2003\n",
      "Epoch 117, Train Loss: 0.3897, Val Loss: 0.2038\n",
      "Epoch 118, Train Loss: 0.3814, Val Loss: 0.2043\n",
      "Epoch 119, Train Loss: 0.4483, Val Loss: 0.2051\n",
      "Epoch 120, Train Loss: 0.5170, Val Loss: 0.2071\n",
      "Epoch 121, Train Loss: 0.4340, Val Loss: 0.2098\n",
      "Epoch 122, Train Loss: 0.4080, Val Loss: 0.2131\n",
      "Epoch 123, Train Loss: 0.4474, Val Loss: 0.2082\n",
      "Epoch 124, Train Loss: 0.3775, Val Loss: 0.2026\n",
      "Epoch 125, Train Loss: 0.3971, Val Loss: 0.2009\n",
      "Early stopping triggered.\n",
      "Final Validation Loss: 0.20032242159630748\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Synthetic Dataset\n",
    "def generate_data(n_samples=200):\n",
    "    X = np.random.randn(n_samples, 10)\n",
    "    true_weights = np.random.randn(10, 1)\n",
    "    y = X @ true_weights + np.random.randn(n_samples, 1) * 0.1\n",
    "    return X, y\n",
    "\n",
    "# 2. BatchNorm Layer\n",
    "class BatchNorm:\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.gamma = np.ones((1, dim))\n",
    "        self.beta = np.zeros((1, dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.mean = np.mean(X, axis=0, keepdims=True)\n",
    "        self.variance = np.var(X, axis=0, keepdims=True)\n",
    "        self.X_norm = (X - self.mean) / np.sqrt(self.variance + self.eps)\n",
    "        return self.gamma * self.X_norm + self.beta\n",
    "\n",
    "# 3. Neural Network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_dim, hidden_dim, use_bn=True, dropout_rate=0.2, lambda_l1=0.0, lambda_l2=0.0):\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, 1) * 0.1\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "\n",
    "        self.use_bn = use_bn\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.lambda_l2 = lambda_l2\n",
    "\n",
    "        if self.use_bn:\n",
    "            self.bn1 = BatchNorm(hidden_dim)\n",
    "\n",
    "    def relu(self, X):\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def dropout(self, X):\n",
    "        self.dropout_mask = (np.random.rand(*X.shape) > self.dropout_rate).astype(float)\n",
    "        return X * self.dropout_mask / (1.0 - self.dropout_rate)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        z1 = X @ self.W1 + self.b1\n",
    "        if self.use_bn:\n",
    "            z1 = self.bn1.forward(z1)\n",
    "        a1 = self.relu(z1)\n",
    "        if training:\n",
    "            a1 = self.dropout(a1)\n",
    "        self.a1 = a1\n",
    "        self.output = a1 @ self.W2 + self.b2\n",
    "        return self.output\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        mse = np.mean((y_true - y_pred) ** 2)\n",
    "        l1 = self.lambda_l1 * (np.sum(np.abs(self.W1)) + np.sum(np.abs(self.W2)))\n",
    "        l2 = self.lambda_l2 * (np.sum(self.W1 ** 2) + np.sum(self.W2 ** 2))\n",
    "        return mse + l1 + l2\n",
    "\n",
    "    def backward(self, X, y, lr=0.01):\n",
    "        m = y.shape[0]\n",
    "        d_output = 2 * (self.output - y) / m\n",
    "\n",
    "        dW2 = self.a1.T @ d_output + self.lambda_l1 * np.sign(self.W2) + 2 * self.lambda_l2 * self.W2\n",
    "        db2 = np.sum(d_output, axis=0, keepdims=True)\n",
    "\n",
    "        da1 = d_output @ self.W2.T\n",
    "        da1[self.a1 <= 0] = 0  # ReLU gradient\n",
    "        da1 *= self.dropout_mask / (1.0 - self.dropout_rate)  # Dropout gradient\n",
    "\n",
    "        dW1 = X.T @ da1 + self.lambda_l1 * np.sign(self.W1) + 2 * self.lambda_l2 * self.W1\n",
    "        db1 = np.sum(da1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "\n",
    "# 4. Early Stopping\n",
    "def early_stopping(val_losses, patience):\n",
    "    if len(val_losses) < patience:\n",
    "        return False\n",
    "    best_loss = min(val_losses[:-patience+1])\n",
    "    return all(loss >= best_loss for loss in val_losses[-patience:])\n",
    "\n",
    "# 5. Training Loop\n",
    "def train(X_train, y_train, X_val, y_val, epochs=1000, patience=10, **nn_kwargs):\n",
    "    model = NeuralNetwork(input_dim=X_train.shape[1], hidden_dim=32, **nn_kwargs)\n",
    "    val_losses = []\n",
    "    best_weights = None\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model.forward(X_train, training=True)\n",
    "        loss = model.compute_loss(y_train, y_pred)\n",
    "        model.backward(X_train, y_train, lr=0.01)\n",
    "\n",
    "        # Validation\n",
    "        y_val_pred = model.forward(X_val, training=False)\n",
    "        val_loss = model.compute_loss(y_val, y_val_pred)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_weights = (model.W1.copy(), model.b1.copy(), model.W2.copy(), model.b2.copy())\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping(val_losses, patience):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Load best weights\n",
    "    model.W1, model.b1, model.W2, model.b2 = best_weights\n",
    "    return model\n",
    "\n",
    "# 6. Run Everything\n",
    "X, y = generate_data()\n",
    "X_train, X_val = X[:150], X[150:]\n",
    "y_train, y_val = y[:150], y[150:]\n",
    "\n",
    "model = train(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    lambda_l1=0.001,\n",
    "    lambda_l2=0.001,\n",
    "    dropout_rate=0.2,\n",
    "    use_bn=True,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "# Test model\n",
    "y_test_pred = model.forward(X_val, training=False)\n",
    "print(\"Final Validation Loss:\", model.compute_loss(y_val, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e644be-4058-475e-bf68-82fcacfddeba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
