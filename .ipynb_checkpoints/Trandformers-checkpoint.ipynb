{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da14526b-40d1-4af3-9ee4-45479065a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "970956eb-36bc-4869-9add-b0c06591fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_embeddings(vocab_size, d_model):\n",
    "    # Random embedding matrix\n",
    "    return np.random.randn(vocab_size, d_model) * 0.01\n",
    "\n",
    "def get_positional_encodings(max_len, d_model):\n",
    "    PE = np.zeros((max_len, d_model))\n",
    "    position = np.arange(0, max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    PE[:, 0::2] = np.sin(position * div_term)\n",
    "    PE[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return PE  # shape (max_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2402c846-7709-48ec-9f6c-ba6918aa5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(d_k)\n",
    "    \n",
    "    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    weights /= np.sum(weights, axis=-1, keepdims=True)\n",
    "    \n",
    "    output = weights @ V\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b21c6e96-39dd-4f47-b8d2-e12dd16340c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Projection matrices\n",
    "        self.W_q = np.random.randn(d_model, d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, X):\n",
    "        # Shape: (batch, seq_len, d_model) → (batch, heads, seq_len, d_k)\n",
    "        B, T, D = X.shape\n",
    "        X = X.reshape(B, T, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "        return X\n",
    "    \n",
    "    def combine_heads(self, X):\n",
    "        # (batch, heads, seq_len, d_k) → (batch, seq_len, d_model)\n",
    "        B, H, T, Dk = X.shape\n",
    "        return X.transpose(0, 2, 1, 3).reshape(B, T, H * Dk)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        Q = X @ self.W_q\n",
    "        K = X @ self.W_k\n",
    "        V = X @ self.W_v\n",
    "        \n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(Q, K, V)\n",
    "        concat = self.combine_heads(attention)\n",
    "        return concat @ self.W_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34674257-710d-41ab-9a9d-2e387978a0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        normalized = (x - mean) / np.sqrt(var + self.eps)\n",
    "        return self.gamma * normalized + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e095d9e-098d-4fd2-b88e-fd75eca7c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model, d_ff)\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model)\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = np.maximum(0, x @ self.W1 + self.b1)  # ReLU\n",
    "        return x @ self.W2 + self.b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efb8e975-165d-4a7c-98bc-d73c7217be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Self-attention + residual\n",
    "        attn_out = self.self_attn(x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feedforward + residual\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28c5048b-fbdf-421a-8e25-0b65ee9ca945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (2, 10, 512)\n"
     ]
    }
   ],
   "source": [
    "def run_encoder_demo():\n",
    "    vocab_size = 10000\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    d_ff = 2048\n",
    "    seq_len = 10\n",
    "    batch_size = 2\n",
    "\n",
    "    # Fake token IDs\n",
    "    tokens = np.random.randint(0, vocab_size, size=(batch_size, seq_len))\n",
    "\n",
    "    # Create embeddings\n",
    "    E = get_token_embeddings(vocab_size, d_model)\n",
    "    PE = get_positional_encodings(seq_len, d_model)\n",
    "\n",
    "    # Embed + add position\n",
    "    X = E[tokens] + PE[np.newaxis, :seq_len, :]\n",
    "\n",
    "    # Encoder\n",
    "    encoder = TransformerEncoderLayer(d_model, num_heads, d_ff)\n",
    "    output = encoder(X)  # shape (batch, seq_len, d_model)\n",
    "\n",
    "    print(\"Encoder output shape:\", output.shape)\n",
    "\n",
    "run_encoder_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3537a-daf8-49a9-876d-e86068502a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
