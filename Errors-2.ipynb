{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c123ceba-8ddf-46f5-ae6c-43060b0c3f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hinge Loss: 0.6000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hinge_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_true: np.array of shape (N,) with values -1 or +1\n",
    "    y_pred: np.array of shape (N,) raw scores from the model\n",
    "\n",
    "    Returns: average hinge loss\n",
    "    \"\"\"\n",
    "    # Ensure y_true and y_pred are numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Compute hinge loss for each example\n",
    "    loss = np.maximum(0, 1 - y_true * y_pred)\n",
    "\n",
    "    # Return average loss\n",
    "    return np.mean(loss)\n",
    "\n",
    "# Example usage:\n",
    "y_true = [1, -1, 1, -1]\n",
    "y_pred = [2.5, -0.3, 0.8, 0.5]  # raw outputs\n",
    "\n",
    "loss = hinge_loss(y_true, y_pred)\n",
    "print(f\"Hinge Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f2cdfa-0ee0-4a23-957a-c0d626626ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Cross-Entropy Loss: 0.4328\n"
     ]
    }
   ],
   "source": [
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    logits: np.array of shape (N, C) where N = number of samples, C = number of classes\n",
    "    Returns: softmax probabilities of shape (N, C)\n",
    "    \"\"\"\n",
    "    exps = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # for numerical stability\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred_probs):\n",
    "    \"\"\"\n",
    "    y_true: np.array of shape (N,) containing true class indices (e.g., 0, 1, 2...)\n",
    "    y_pred_probs: np.array of shape (N, C), softmax probabilities\n",
    "\n",
    "    Returns: average cross-entropy loss\n",
    "    \"\"\"\n",
    "    N = y_true.shape[0]\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred_probs = np.clip(y_pred_probs, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    # Pick the probability of the true class for each sample\n",
    "    correct_probs = y_pred_probs[np.arange(N), y_true]\n",
    "\n",
    "    # Take negative log\n",
    "    loss = -np.log(correct_probs)\n",
    "\n",
    "    return np.mean(loss)\n",
    "\n",
    "# Example usage:\n",
    "logits = np.array([\n",
    "    [2.0, 1.0, 0.1],\n",
    "    [0.5, 2.2, 1.5],\n",
    "    [1.2, 0.7, 2.5]\n",
    "])\n",
    "y_true = np.array([0, 1, 2])  # true class indices\n",
    "\n",
    "probs = softmax(logits)\n",
    "loss = categorical_cross_entropy(y_true, probs)\n",
    "print(f\"Categorical Cross-Entropy Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fb844-4112-4185-9ab3-f68a6fd3fef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
